{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b11d478",
   "metadata": {},
   "source": [
    "The goal of this project is to write a data story on philosophy using the dataset for the Philosophy Data Project. Applying data mining, statistical analysis and visualization, students should derive interesting findings in this collection of philosophy texts and write a \"data story\" that can be shared with a general audience.\n",
    "\n",
    "Challenge\n",
    "In this project you will carry out an exploratory data analysis (EDA) of philosophy texts and write a blog on interesting findings from your analysis (i.e., a data story).\n",
    "\n",
    "You are tasked to explore the text corpus using tools from data mining, statistical analysis and visualization, etc, all available in R or Python and write a blog post using R or Python Notebook. Your blog should be in the form of a data story blog on interesting trends and patterns identified by your analysis of these philosophy texts.\n",
    "\n",
    "Even though this is an individual project, you are encouraged to discuss with your classmates and exchange ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8692ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import wikipedia\n",
    "\n",
    "class Utils:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "    \n",
    "    def read_data(self):\n",
    "        self.data = pd.read_csv('../data/philosophy_data.csv')\n",
    "        self.author_list = list(self.data.author.unique())\n",
    "        self.author_list.sort()\n",
    "        self.school_list = list(self.data.school.unique())\n",
    "        self.school_list.sort()\n",
    "        \n",
    "        self.author_works_dict = {a: [] for a in self.author_list}\n",
    "        for index, row in self.data[['author','title']].drop_duplicates().reset_index(drop=True).iterrows():\n",
    "            auth = row['author']\n",
    "            titl = row['title']\n",
    "            titl = titl.replace(auth, '')\n",
    "            self.author_works_dict[auth].append(re.sub('[^A-Za-z0-9]+', ' ', titl.lower()))\n",
    "        \n",
    "        self.author_school_dict = {\n",
    "            a: list(self.data[self.data.author == a].school.unique())[0].lower() for a in self.author_list\n",
    "        }\n",
    "            \n",
    "#     def describe_data(self):\n",
    "        # TO DO - describe data\n",
    "    \n",
    "    ### CLEANING FUNCS ###\n",
    "    def rem_sw(self, var_in):\n",
    "        sw = stopwords.words('english')    \n",
    "        clean_text = [word for word in var_in.split() if word not in sw]\n",
    "        clean_text = ' '.join(clean_text)\n",
    "        return clean_text\n",
    "    \n",
    "    def clean_text(self, var_in):\n",
    "        tmp = re.sub(\"[^A-Za-z]+\", \" \", var_in.lower())\n",
    "        return tmp\n",
    "    \n",
    "    def stem_fun(self, var):\n",
    "        from nltk.stem import PorterStemmer\n",
    "        my_stem = PorterStemmer()\n",
    "        tmp = [my_stem.stem(word) for word in var.split()]\n",
    "        tmp = ' '.join(tmp)\n",
    "        return tmp\n",
    "    \n",
    "    def add_clean_cols_to_data(self):\n",
    "        self.data[\"sentence_lowered\"] = self.data.sentence_str.apply(\n",
    "            lambda x: x.lower()\n",
    "        )\n",
    "        self.data[\"clean_text\"] = self.data.sentence_lowered.apply(self.clean_text)\n",
    "        self.data[\"rem_sw\"] = self.data.clean_text.apply(self.rem_sw)\n",
    "        self.data[\"rem_sw_stem\"] = self.data.rem_sw.apply(self.stem_fun)\n",
    "#         return self.data\n",
    "\n",
    "    ### SENTIMENT ANALYSIS ###\n",
    "    def get_sentiment_words(self):\n",
    "        # TO DO - change to rel path\n",
    "        file_names = ['positive-words', 'negative-words']\n",
    "        pos_neg_dict = {}\n",
    "        for file in file_names:\n",
    "            path = \"/Users/melissa/Desktop/columbia/class/Applied_Data_Science/Fall2021-Project1-melrbischoff/data/{}.txt\".format(\n",
    "                file\n",
    "            )\n",
    "            with open(path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "                contents = []\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    contents.append(line)\n",
    "            f.close()\n",
    "            pos_neg_dict[file] = contents\n",
    "\n",
    "    def gen_senti(self, arbitrary_text):\n",
    "        '''\n",
    "        Tokenizes arbitrary text and compares each token with the positive and \n",
    "        negative lexicons of each dictionary and outputs the sentiment score, S\n",
    "        '''\n",
    "        import re\n",
    "        arbitrary_text_clean = re.sub(r'[^a-zA-Z ]+', '', arbitrary_text)\n",
    "        arbitrary_text_list = arbitrary_text_clean.split()\n",
    "\n",
    "        pw = [-1 for word in arbitrary_text_list if word in (pos_neg_dict['negative-words'])]\n",
    "        nw = [1 for word in arbitrary_text_list if word in (pos_neg_dict['positive-words'])]\n",
    "        pc = len(pw)\n",
    "        nc = len(nw)\n",
    "        total = pc + nc\n",
    "        try:\n",
    "            S = (sum(pw) + sum(nw)) / total\n",
    "        except ZeroDivisionError:\n",
    "            S = None\n",
    "        return S\n",
    "    \n",
    "    def vader_senti(self):\n",
    "        vaderSent = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def gen_textblob_senti(self, var_in):\n",
    "        blob = TextBlob(var_in)\n",
    "        return blob.sentiment.polarity\n",
    "    \n",
    "    def run_sentiment_analysis(self):\n",
    "        self.data['simple_senti'] = self.data.rem_sw_stem.apply(gen_senti)\n",
    "        self.data['vader'] = self.data.rem_sw_stem.apply(\n",
    "            lambda x: vaderSent.polarity_scores(x)['compound']\n",
    "        )\n",
    "        self.data['textblob_senti'] = self.data.rem_sw_stem.apply(gen_textblob_senti)\n",
    "    \n",
    "    ### WIKIPEDIA DATA ###\n",
    "    def get_author_wikipedia_page(self):\n",
    "        self.author_wiki_dict = {}\n",
    "\n",
    "        for author in self.author_list:\n",
    "            author_titles = self.author_works_dict[author]\n",
    "            author_school = self.author_school_dict[author]\n",
    "\n",
    "            all_wiki_titles = wikipedia.search(f'philosopher {author} person')\n",
    "            wiki_author_titles = [x for x in all_wiki_titles if author.lower() in x.lower() and 'surname' not in x]\n",
    "\n",
    "            try:\n",
    "                t = wiki_author_titles[0]\n",
    "                content = (wikipedia.WikipediaPage(t).content).lower()\n",
    "\n",
    "                boolean_ct = []\n",
    "                for title in author_titles:\n",
    "                    boolean_ct.append(str(title in content))\n",
    "\n",
    "                if (('True' in boolean_ct) or (author_school in content)):\n",
    "                    self.author_wiki_dict[author] = content\n",
    "                else:\n",
    "                    self.author_wiki_dict[author] = 'cant find page'\n",
    "\n",
    "            except IndexError:\n",
    "                self.author_wiki_dict[author] = 'cant find title'\n",
    "        \n",
    "        missing_authors = [key for key, value in self.author_wiki_dict.items() if value == 'cant find page' or value == 'cant find title']\n",
    "\n",
    "        for author in missing_authors:\n",
    "            author_school = self.author_school_dict[author]\n",
    "            author_titles = self.author_works_dict[author]\n",
    "\n",
    "            all_titles = wikipedia.search(f'{author} philosopher')\n",
    "            wiki_titles = [x for x in all_titles if author.lower() in x.lower() and 'surname' not in x]\n",
    "\n",
    "            try:\n",
    "                t = wiki_titles[0]\n",
    "                content = (wikipedia.WikipediaPage(t).content).lower()\n",
    "\n",
    "                boolean_ct = []\n",
    "                for title in author_titles:\n",
    "                    boolean_ct.append(str(title in content))\n",
    "\n",
    "                if ('True' in boolean_ct) or (author_school in content.lower()):\n",
    "                    self.author_wiki_dict[author] = content\n",
    "                else:\n",
    "                    self.author_wiki_dict[author] = 'cant find page'\n",
    "\n",
    "            except IndexError:\n",
    "                self.author_wiki_dict[author] = 'cant find title'\n",
    "        \n",
    "        missing_authors_final = [key for key, value in self.author_wiki_dict.items() if value == 'cant find page' or value == 'cant find title']\n",
    "        \n",
    "        if len(missing_authors_final) > 1:\n",
    "            print('cant find wikipedia page for authors: {}'.format(', '.join(missing_authors_final)))\n",
    "        if len(missing_authors_final) == 1:\n",
    "            print(f'cant find wikipedia page for author {missing_authors_final[0]}')\n",
    "        \n",
    "        # remove missing authors from dict\n",
    "        for author in missing_authors_final:\n",
    "            self.author_wiki_dict.pop(author)\n",
    "    \n",
    "    def get_author_sexes(self):\n",
    "        female_pronouns = ['she','her','hers']\n",
    "        male_pronouns = ['he','him','his']\n",
    "        \n",
    "        self.sex_dict = {}\n",
    "        for author, content in self.author_wiki_dict.items():\n",
    "            female = sum([1 for word in content.split() if word in (female_pronouns)])\n",
    "            male = sum([1 for word in content.split() if word in (male_pronouns)])\n",
    "            if abs(female - male) < 5:\n",
    "                self.sex_dict[author] = {'female': female,\n",
    "                                    'male': male}\n",
    "            elif female > male:\n",
    "                self.sex_dict[author] = 'female'\n",
    "            elif male > female:\n",
    "                self.sex_dict[author] = 'male'\n",
    "    \n",
    "    def get_school_wikipedia_page(self):\n",
    "        # TO DO this isn't finished\n",
    "        self.school_wiki_dict = {}\n",
    "\n",
    "        for school in self.school_list:\n",
    "            all_wiki_titles = wikipedia.search(f'{school} school of philosophy')\n",
    "            try:\n",
    "                t = all_wiki_titles[0]\n",
    "                content = (wikipedia.WikipediaPage(t).content).lower()\n",
    "\n",
    "                self.school_wiki_dict[school] = content\n",
    "\n",
    "            except IndexError:\n",
    "                print(f'cant even find title for {school}')\n",
    "                self.school_wiki_dict[school] = 'cant find title'\n",
    "    \n",
    "    ### TOPIC MODELING ###\n",
    "    def lda_fun(self, df_in, n_topics_in, num_words_in):\n",
    "        data_tmp = df_in.str.split()\n",
    "        id2word = corpora.Dictionary(data_tmp)\n",
    "\n",
    "        corpus = [id2word.doc2bow(text) for text in data_tmp]\n",
    "\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(\n",
    "            corpus, num_topics=n_topics_in, id2word=id2word, passes=15)\n",
    "        ldamodel.save('model5.gensim')\n",
    "        topics = ldamodel.print_topics(num_words=num_words_in)\n",
    "#         print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  \n",
    "        coherence_model_lda = CoherenceModel(\n",
    "            model=ldamodel, texts=data_tmp, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "#         print('\\nCoherence Score: ', coherence_lda)\n",
    "        for topic in topics:\n",
    "            print(topic)\n",
    "        return topics\n",
    "    \n",
    "    def run_and_write_lda_authors_fun(self):\n",
    "        self.lda_author_topics_dict = {}\n",
    "        \n",
    "        for author in self.author_list:\n",
    "            the_topics = lda_fun(self.data[self.data['author'] == author].rem_sw_stem, 5, 4)\n",
    "            self.lda_author_topics_dict[author] = the_topics\n",
    "        # write lda_author_topics_dict to output\n",
    "        self.lda_author_topics_df = pd.DataFrame(lda_author_topics_dict)\n",
    "        self.lda_author_topics_df.to_csv('../output/lda_author_topics_df.csv')\n",
    "    \n",
    "    def run_and_write_lda_school_fun(self):\n",
    "        self.lda_school_topics_dict = {}\n",
    "        for school in school_list:\n",
    "            the_topics = lda_fun(self.data[self.data['school'] == school].rem_sw_stem, 5, 4)\n",
    "            self.lda_school_topics_dict[school] = the_topics\n",
    "        # write lda_school_topics_df to output\n",
    "        self.lda_school_topics_df = pd.DataFrame(lda_school_topics_dict)\n",
    "        self.lda_school_topics_df.to_csv('../output/lda_school_topics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a1117dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Utils(dataset_name='philosophy_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69909eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dd96bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner.add_clean_cols_to_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90ce917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cant find wikipedia page for author Ricardo\n"
     ]
    }
   ],
   "source": [
    "runner.get_author_wikipedia_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e7d8d",
   "metadata": {},
   "source": [
    "# overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_overlap_dict = dict.fromkeys(author_list)\n",
    "for author, content in author_wiki_dict:\n",
    "    for ref_author in author_list:\n",
    "        cnt = sum([1 for word in content.split() if ref_author in word])\n",
    "        author_overlap_dict[author][ref_author] = cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259bcf6",
   "metadata": {},
   "source": [
    "# dont have a place for these yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea8d72",
   "metadata": {},
   "source": [
    "# dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e01f814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_dates = data[['author','original_publication_date']].groupby(['author']).max('original_publication_date').join(\n",
    "    data[['author','original_publication_date']].groupby(['author']).min('original_publication_date'),\n",
    "    lsuffix = '_max',\n",
    "    rsuffix = '_min'\n",
    ")\n",
    "\n",
    "author_dates.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9fc38aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lewis - Papers'], dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.author == 'Lewis'].title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd7da46d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>original_publication_date_max</th>\n",
       "      <th>original_publication_date_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>-320</td>\n",
       "      <td>-320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beauvoir</td>\n",
       "      <td>1949</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Berkeley</td>\n",
       "      <td>1713</td>\n",
       "      <td>1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Davis</td>\n",
       "      <td>1981</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deleuze</td>\n",
       "      <td>1972</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Derrida</td>\n",
       "      <td>1967</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Descartes</td>\n",
       "      <td>1641</td>\n",
       "      <td>1637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Epictetus</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fichte</td>\n",
       "      <td>1798</td>\n",
       "      <td>1798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Foucault</td>\n",
       "      <td>1966</td>\n",
       "      <td>1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hegel</td>\n",
       "      <td>1820</td>\n",
       "      <td>1807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Heidegger</td>\n",
       "      <td>1950</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hume</td>\n",
       "      <td>1779</td>\n",
       "      <td>1739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Husserl</td>\n",
       "      <td>1936</td>\n",
       "      <td>1907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kant</td>\n",
       "      <td>1790</td>\n",
       "      <td>1781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Keynes</td>\n",
       "      <td>1936</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kripke</td>\n",
       "      <td>1975</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Leibniz</td>\n",
       "      <td>1710</td>\n",
       "      <td>1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lenin</td>\n",
       "      <td>1862</td>\n",
       "      <td>1862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lewis</td>\n",
       "      <td>1985</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Locke</td>\n",
       "      <td>1689</td>\n",
       "      <td>1689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Malebranche</td>\n",
       "      <td>1674</td>\n",
       "      <td>1674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Marcus Aurelius</td>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Marx</td>\n",
       "      <td>1883</td>\n",
       "      <td>1848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Merleau-Ponty</td>\n",
       "      <td>1945</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Moore</td>\n",
       "      <td>1910</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nietzsche</td>\n",
       "      <td>1888</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Plato</td>\n",
       "      <td>-350</td>\n",
       "      <td>-350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Popper</td>\n",
       "      <td>1959</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Quine</td>\n",
       "      <td>1950</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ricardo</td>\n",
       "      <td>1817</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Russell</td>\n",
       "      <td>1921</td>\n",
       "      <td>1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Smith</td>\n",
       "      <td>1776</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Spinoza</td>\n",
       "      <td>1677</td>\n",
       "      <td>1677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Wittgenstein</td>\n",
       "      <td>1953</td>\n",
       "      <td>1921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Wollstonecraft</td>\n",
       "      <td>1792</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author  original_publication_date_max  \\\n",
       "0         Aristotle                           -320   \n",
       "1          Beauvoir                           1949   \n",
       "2          Berkeley                           1713   \n",
       "3             Davis                           1981   \n",
       "4           Deleuze                           1972   \n",
       "5           Derrida                           1967   \n",
       "6         Descartes                           1641   \n",
       "7         Epictetus                            125   \n",
       "8            Fichte                           1798   \n",
       "9          Foucault                           1966   \n",
       "10            Hegel                           1820   \n",
       "11        Heidegger                           1950   \n",
       "12             Hume                           1779   \n",
       "13          Husserl                           1936   \n",
       "14             Kant                           1790   \n",
       "15           Keynes                           1936   \n",
       "16           Kripke                           1975   \n",
       "17          Leibniz                           1710   \n",
       "18            Lenin                           1862   \n",
       "19            Lewis                           1985   \n",
       "20            Locke                           1689   \n",
       "21      Malebranche                           1674   \n",
       "22  Marcus Aurelius                            170   \n",
       "23             Marx                           1883   \n",
       "24    Merleau-Ponty                           1945   \n",
       "25            Moore                           1910   \n",
       "26        Nietzsche                           1888   \n",
       "27            Plato                           -350   \n",
       "28           Popper                           1959   \n",
       "29            Quine                           1950   \n",
       "30          Ricardo                           1817   \n",
       "31          Russell                           1921   \n",
       "32            Smith                           1776   \n",
       "33          Spinoza                           1677   \n",
       "34     Wittgenstein                           1953   \n",
       "35   Wollstonecraft                           1792   \n",
       "\n",
       "    original_publication_date_min  \n",
       "0                            -320  \n",
       "1                            1949  \n",
       "2                            1710  \n",
       "3                            1981  \n",
       "4                            1968  \n",
       "5                            1967  \n",
       "6                            1637  \n",
       "7                             125  \n",
       "8                            1798  \n",
       "9                            1961  \n",
       "10                           1807  \n",
       "11                           1927  \n",
       "12                           1739  \n",
       "13                           1907  \n",
       "14                           1781  \n",
       "15                           1936  \n",
       "16                           1972  \n",
       "17                           1710  \n",
       "18                           1862  \n",
       "19                           1985  \n",
       "20                           1689  \n",
       "21                           1674  \n",
       "22                            170  \n",
       "23                           1848  \n",
       "24                           1945  \n",
       "25                           1910  \n",
       "26                           1886  \n",
       "27                           -350  \n",
       "28                           1959  \n",
       "29                           1950  \n",
       "30                           1817  \n",
       "31                           1912  \n",
       "32                           1776  \n",
       "33                           1677  \n",
       "34                           1921  \n",
       "35                           1792  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fddfb6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simple_senti</th>\n",
       "      <th>vader</th>\n",
       "      <th>textblob_senti</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>continental</th>\n",
       "      <td>-0.221889</td>\n",
       "      <td>-0.002555</td>\n",
       "      <td>0.005911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytic</th>\n",
       "      <td>-0.067521</td>\n",
       "      <td>0.078045</td>\n",
       "      <td>0.026725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phenomenology</th>\n",
       "      <td>-0.124603</td>\n",
       "      <td>0.083608</td>\n",
       "      <td>0.024081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communism</th>\n",
       "      <td>0.098497</td>\n",
       "      <td>0.084579</td>\n",
       "      <td>0.032395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feminism</th>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.093631</td>\n",
       "      <td>0.050450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               simple_senti     vader  textblob_senti\n",
       "school                                               \n",
       "continental       -0.221889 -0.002555        0.005911\n",
       "analytic          -0.067521  0.078045        0.026725\n",
       "phenomenology     -0.124603  0.083608        0.024081\n",
       "communism          0.098497  0.084579        0.032395\n",
       "feminism           0.005502  0.093631        0.050450"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg senti by school\n",
    "data.groupby('school').mean('textblob_senti')[['simple_senti','vader','textblob_senti']].sort_values('vader').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "633f02cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simple_senti</th>\n",
       "      <th>vader</th>\n",
       "      <th>textblob_senti</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Davis</th>\n",
       "      <td>-0.113652</td>\n",
       "      <td>-0.078276</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foucault</th>\n",
       "      <td>-0.210015</td>\n",
       "      <td>-0.020949</td>\n",
       "      <td>-0.017650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Derrida</th>\n",
       "      <td>-0.133301</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deleuze</th>\n",
       "      <td>-0.279859</td>\n",
       "      <td>0.013206</td>\n",
       "      <td>0.031775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epictetus</th>\n",
       "      <td>-0.063974</td>\n",
       "      <td>0.040315</td>\n",
       "      <td>0.018255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           simple_senti     vader  textblob_senti\n",
       "author                                           \n",
       "Davis         -0.113652 -0.078276        0.023300\n",
       "Foucault      -0.210015 -0.020949       -0.017650\n",
       "Derrida       -0.133301  0.011229        0.011700\n",
       "Deleuze       -0.279859  0.013206        0.031775\n",
       "Epictetus     -0.063974  0.040315        0.018255"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('author').mean('textblob_senti')[['simple_senti','vader','textblob_senti']].sort_values('vader').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions from utils.py\n",
    "def tf_idf_fun(df_in, path_in, name_in):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import pandas as pd\n",
    "    my_tf_idf = TfidfVectorizer()\n",
    "    my_tf_idf_text = pd.DataFrame(my_tf_idf.fit_transform(df_in).toarray())\n",
    "    my_tf_idf_text.columns = my_tf_idf.get_feature_names()\n",
    "        \n",
    "    write_pickle(path_in + \"output/\", name_in + \".pkl\", my_tf_idf)\n",
    "    return my_tf_idf_text\n",
    "\n",
    "def vec_fun(df_in, path_in, name_in):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import pandas as pd\n",
    "    my_vec = CountVectorizer()\n",
    "    \n",
    "    my_vec_text = pd.DataFrame(my_vec.fit_transform(df_in).toarray())\n",
    "    my_vec_text.columns = my_vec.get_feature_names()\n",
    "    \n",
    "    write_pickle(path_in + \"output/\", name_in + \".pkl\", my_vec)\n",
    "    return my_vec_text\n",
    "\n",
    "def tf_idf_fun(df_in, path_in, name_in):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import pandas as pd\n",
    "    my_tf_idf = TfidfVectorizer()\n",
    "    my_tf_idf_text = pd.DataFrame(my_tf_idf.fit_transform(df_in).toarray())\n",
    "    my_tf_idf_text.columns = my_tf_idf.get_feature_names()\n",
    "    return my_tf_idf_text\n",
    "\n",
    "def grid_search_fun(x_in, y_in, params_in, sw):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.svm import SVC\n",
    "    if sw == \"rf\":\n",
    "        my_rf = RandomForestClassifier(random_state=123)\n",
    "    elif sw == \"svm\":\n",
    "        my_rf = SVC(random_state=123)\n",
    "    elif sw == \"nb\":\n",
    "        my_rf = MultinomialNB()\n",
    "    clf = GridSearchCV(my_rf, params_in)\n",
    "    clf.fit(x_in, y_in)\n",
    "    print (\"Best Score:\", clf.best_score_, \"Best Params:\", clf.best_params_)\n",
    "    return clf.best_params_\n",
    "\n",
    "def pca_fun(var, exp_var, path_o):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=exp_var)\n",
    "    pca_data = pca.fit_transform(var)\n",
    "    write_pickle(path_o, \"pca.pkl\", pca)\n",
    "    print(\"# components:\", len(pca.explained_variance_ratio_))\n",
    "    print(\"explained variance:\",sum(pca.explained_variance_ratio_))\n",
    "    return pca_data\n",
    "\n",
    "def perf_metrics(model_in, x_in, y_true):\n",
    "    #How well did this model perform?\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    y_pred = model_in.predict(x_in)\n",
    "    metrics = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted')\n",
    "    return metrics\n",
    "\n",
    "def my_rf(x_in, y_in, out_in, opt_param_in, sw):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    if sw == \"rf\":\n",
    "        my_rf_m = RandomForestClassifier(**opt_param_in)\n",
    "    elif sw == \"svm\":\n",
    "        my_rf_m = SVC(**opt_param_in)\n",
    "    elif sw == \"nb\":\n",
    "        my_rf_m = MultinomialNB(**opt_param_in)\n",
    "    my_rf_m.fit(x_in, y_in) #model is trained\n",
    "    write_pickle(out_in, \"rf.pkl\", my_rf_m)\n",
    "    return my_rf_m\n",
    "\n",
    "def split_data(x_in, y_in, split_fraction):\n",
    "    # training test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
    "        x_in, y_in, test_size=split_fraction, random_state=42)\n",
    "    return X_train_t, X_test_t, y_train_t, y_test_t\n",
    "\n",
    "def my_cos_fun(df_in, xform_in, label_in):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import pandas as pd\n",
    "    similarity = pd.DataFrame(cosine_similarity(df_in, xform_in))\n",
    "    similarity.index = label_in\n",
    "    return similarity\n",
    "\n",
    "def my_pca(df_in, o_path):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=0.95)\n",
    "    my_pca_txt = pca.fit_transform(df_in)\n",
    "    write_pickle(o_path, \"pca.pkl\", pca)\n",
    "    return my_pca_txt\n",
    "\n",
    "def score_text(model_in, var_in):\n",
    "    import numpy as np\n",
    "    the_pred = model_in.predict(var_in)\n",
    "    probs = model_in.predict_proba(var_in)\n",
    "    print (\"Predicted text:\", the_pred[0], \"With probability of:\",\n",
    "           str(round(np.max(probs)*100, 2)) + \"%\")\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
