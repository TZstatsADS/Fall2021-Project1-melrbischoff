{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea6535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install re\n",
    "!pip3 install gensim\n",
    "!pip3 install bs4\n",
    "!pip3 install requests\n",
    "!pip3 install fake_useragent\n",
    "!pip3 install matplotlib_venn_wordcloud\n",
    "!pip3 install wordcloud\n",
    "!pip3 install pillow\n",
    "!pip3 install wikipedia\n",
    "!pip3 install matplotlib\n",
    "!pip3 install numpy\n",
    "!pip3 install dataframe-image\n",
    "!pip3 install nltk\n",
    "!pip3 install textblob\n",
    "!pip3 install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8692ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from matplotlib_venn_wordcloud import venn2_wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import wikipedia\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import dataframe_image as dfi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ebe99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/philosophy_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa841d",
   "metadata": {},
   "source": [
    "# define cleaning funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74532a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_output(df_in, df_name):\n",
    "    path = f'../output/{df_name}.csv'\n",
    "    df_in.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c88208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_sw(var_in):\n",
    "    sw = stopwords.words('english')    \n",
    "    clean_text = [word for word in var_in.split() if word not in sw]\n",
    "    clean_text = ' '.join(clean_text)\n",
    "    return clean_text\n",
    "  \n",
    "def clean_text(var_in):\n",
    "    tmp = re.sub(\"[^A-Za-z]+\", \" \", var_in.lower())\n",
    "    return tmp\n",
    "\n",
    "def stem_fun(var):\n",
    "    my_stem = PorterStemmer()\n",
    "    tmp = [my_stem.stem(word) for word in var.split()]\n",
    "    tmp = ' '.join(tmp)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660ecce",
   "metadata": {},
   "source": [
    "# get wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0679c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = list(data.author.unique())\n",
    "author_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e89a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_works_dict = {a: [] for a in author_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e872055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data[['author','title']].drop_duplicates().reset_index(drop=True).iterrows():\n",
    "    auth = row['author']\n",
    "    titl = row['title']\n",
    "    titl = titl.replace(auth, '')\n",
    "    author_works_dict[auth].append(re.sub('[^A-Za-z0-9]+', ' ', titl.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442fb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_school_dict = {a: list(data[data.author == a].school.unique())[0].lower() for a in author_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c4efb75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running author: Aristotle\n",
      "page title: Aristotle\n",
      "found page for Aristotle, it is Aristotle\n",
      "running author: Beauvoir\n",
      "page title: Simone de Beauvoir\n",
      "found page for Beauvoir, it is Simone de Beauvoir\n",
      "running author: Berkeley\n",
      "page title: George Berkeley\n",
      "found page for Berkeley, it is George Berkeley\n",
      "running author: Davis\n",
      "page title: Angela Davis\n",
      "found page for Davis, it is Angela Davis\n",
      "running author: Deleuze\n",
      "page title: Gilles Deleuze\n",
      "found page for Deleuze, it is Gilles Deleuze\n",
      "running author: Derrida\n",
      "page title: Derrida\n",
      "CANT FIND page for Derrida\n",
      "running author: Descartes\n",
      "page title: René Descartes\n",
      "found page for Descartes, it is René Descartes\n",
      "running author: Epictetus\n",
      "page title: Epictetus\n",
      "found page for Epictetus, it is Epictetus\n",
      "running author: Fichte\n",
      "page title: Johann Gottlieb Fichte\n",
      "found page for Fichte, it is Johann Gottlieb Fichte\n",
      "running author: Foucault\n",
      "page title: Michel Foucault\n",
      "found page for Foucault, it is Michel Foucault\n",
      "running author: Hegel\n",
      "page title: Georg Wilhelm Friedrich Hegel\n",
      "found page for Hegel, it is Georg Wilhelm Friedrich Hegel\n",
      "running author: Heidegger\n",
      "page title: Heideggerian terminology\n",
      "found page for Heidegger, it is Heideggerian terminology\n",
      "running author: Hume\n",
      "page title: David Hume\n",
      "found page for Hume, it is David Hume\n",
      "running author: Husserl\n",
      "page title: Edmund Husserl\n",
      "found page for Husserl, it is Edmund Husserl\n",
      "running author: Kant\n",
      "page title: Immanuel Kant\n",
      "found page for Kant, it is Immanuel Kant\n",
      "running author: Keynes\n",
      "page title: John Maynard Keynes\n",
      "found page for Keynes, it is John Maynard Keynes\n",
      "running author: Kripke\n",
      "page title: Saul Kripke\n",
      "found page for Kripke, it is Saul Kripke\n",
      "running author: Leibniz\n",
      "page title: Gottfried Wilhelm Leibniz\n",
      "found page for Leibniz, it is Gottfried Wilhelm Leibniz\n",
      "running author: Lenin\n",
      "page title: Vladimir Lenin\n",
      "found page for Lenin, it is Vladimir Lenin\n",
      "running author: Lewis\n",
      "page title: David Lewis (philosopher)\n",
      "found page for Lewis, it is David Lewis (philosopher)\n",
      "running author: Locke\n",
      "page title: John Locke\n",
      "found page for Locke, it is John Locke\n",
      "running author: Malebranche\n",
      "cant even find title for Malebranche\n",
      "running author: Marcus Aurelius\n",
      "page title: Marcus Aurelius\n",
      "found page for Marcus Aurelius, it is Marcus Aurelius\n",
      "running author: Marx\n",
      "page title: Karl Marx\n",
      "found page for Marx, it is Karl Marx\n",
      "running author: Merleau-Ponty\n",
      "page title: Maurice Merleau-Ponty\n",
      "found page for Merleau-Ponty, it is Maurice Merleau-Ponty\n",
      "running author: Moore\n",
      "page title: A. W. Moore (philosopher)\n",
      "found page for Moore, it is A. W. Moore (philosopher)\n",
      "running author: Nietzsche\n",
      "page title: Friedrich Nietzsche\n",
      "found page for Nietzsche, it is Friedrich Nietzsche\n",
      "running author: Plato\n",
      "page title: Plato\n",
      "found page for Plato, it is Plato\n",
      "running author: Popper\n",
      "cant even find title for Popper\n",
      "running author: Quine\n",
      "cant even find title for Quine\n",
      "running author: Ricardo\n",
      "cant even find title for Ricardo\n",
      "running author: Russell\n",
      "page title: Russell's teapot\n",
      "CANT FIND page for Russell\n",
      "running author: Smith\n",
      "page title: Adam Smith\n",
      "found page for Smith, it is Adam Smith\n",
      "running author: Spinoza\n",
      "page title: Baruch Spinoza\n",
      "found page for Spinoza, it is Baruch Spinoza\n",
      "running author: Wittgenstein\n",
      "page title: Ludwig Wittgenstein\n",
      "found page for Wittgenstein, it is Ludwig Wittgenstein\n",
      "running author: Wollstonecraft\n",
      "page title: Mary Wollstonecraft\n",
      "found page for Wollstonecraft, it is Mary Wollstonecraft\n"
     ]
    }
   ],
   "source": [
    "author_wiki_dict = {}\n",
    "\n",
    "for author in author_list:\n",
    "    print('running author: ' + author)\n",
    "    author_titles = author_works_dict[author]\n",
    "    author_school = author_school_dict[author]\n",
    "    \n",
    "    all_wiki_titles = wikipedia.search(f'philosopher {author} person')\n",
    "    wiki_author_titles = [x for x in all_wiki_titles if author.lower() in x.lower() and 'surname' not in x]\n",
    "    \n",
    "    try:\n",
    "        t = wiki_author_titles[0]\n",
    "        print('page title: ' + t)\n",
    "        content = (wikipedia.WikipediaPage(t).content).lower()\n",
    "            \n",
    "        boolean_ct = []\n",
    "        for title in author_titles:\n",
    "            boolean_ct.append(str(title in content))\n",
    "\n",
    "        if (('True' in boolean_ct) or (author_school in content)):\n",
    "            print(f'found page for {author}, it is {t}')\n",
    "            author_wiki_dict[author] = content\n",
    "        else:\n",
    "            print(f'CANT FIND page for {author}')\n",
    "            author_wiki_dict[author] = 'cant find page'\n",
    "        \n",
    "    except IndexError:\n",
    "        print(f'cant even find title for {author}')\n",
    "        author_wiki_dict[author] = 'cant find title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2293363",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_authors = [key for key, value in author_wiki_dict.items() if value == 'cant find page' or value == 'cant find title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48b36466",
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in missing_authors:\n",
    "#     print('running author: ' + author)\n",
    "    author_school = author_school_dict[author]\n",
    "    author_titles = author_works_dict[author]\n",
    "    \n",
    "    all_titles = wikipedia.search(f'{author} philosopher')\n",
    "    wiki_titles = [x for x in all_titles if author.lower() in x.lower() and 'surname' not in x]\n",
    "    \n",
    "    try:\n",
    "        t = wiki_titles[0]\n",
    "#         print('page title: ' + wiki_titles[0])\n",
    "        content = (wikipedia.WikipediaPage(t).content).lower()\n",
    "            \n",
    "        boolean_ct = []\n",
    "        for title in author_titles:\n",
    "            boolean_ct.append(str(title in content))\n",
    "\n",
    "        if ('True' in boolean_ct) or (author_school in content.lower()):\n",
    "#             print(f'found page for {author}, it is {wiki_titles[0]}')\n",
    "            author_wiki_dict[author] = content\n",
    "        else:\n",
    "#             print(f'CANT FIND page for {author}')\n",
    "            author_wiki_dict[author] = 'cant find page'\n",
    "        \n",
    "    except IndexError:\n",
    "#         print(f'cant even find title for {author}')\n",
    "        author_wiki_dict[author] = 'cant find title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efbf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_authors_final = [key for key, value in author_wiki_dict.items() if value == 'cant find page' or value == 'cant find title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ffb0717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cant find wikipedia page for author Ricardo\n"
     ]
    }
   ],
   "source": [
    "if len(missing_authors_final) > 1:\n",
    "    print('cant find wikipedia page for authors: {}'.format(', '.join(missing_authors_final)))\n",
    "if len(missing_authors_final) == 1:\n",
    "    print(f'cant find wikipedia page for author {missing_authors_final[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b3ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing authors from dict\n",
    "for author in missing_authors_final:\n",
    "    author_wiki_dict.pop(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fefa4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing authors from author list\n",
    "for author in missing_authors_final:\n",
    "    author_list.remove(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9973f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing authors from data\n",
    "data = data[~data.author.isin(missing_authors_final)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b269d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleaning functions to author wiki pages\n",
    "author_wiki_df = pd.DataFrame(author_wiki_dict.items(), columns = ['author','content'])\n",
    "author_wiki_df = author_wiki_df[~author_wiki_df.author.isin(missing_authors_final)]\n",
    "author_wiki_df[\"content_lowered\"] = author_wiki_df.content.apply(\n",
    "    lambda x: x.lower()\n",
    ")\n",
    "author_wiki_df[\"clean_text\"] = author_wiki_df.content_lowered.apply(clean_text)\n",
    "author_wiki_df[\"rem_sw\"] = author_wiki_df.clean_text.apply(rem_sw)\n",
    "author_wiki_df[\"rem_sw_stem\"] = author_wiki_df.rem_sw.apply(stem_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a983c8",
   "metadata": {},
   "source": [
    "# get author sexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2faf76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_dict = {a: [] for a in author_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d68f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_pronouns = ['she','her','hers']\n",
    "male_pronouns = ['he','him','his']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21ec317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_dict = {}\n",
    "for author, content in author_wiki_dict.items():\n",
    "    female = sum([1 for word in content.split() if word in (female_pronouns)])\n",
    "    male = sum([1 for word in content.split() if word in (male_pronouns)])\n",
    "    if abs(female - male) < 5:\n",
    "        print(f'author {author} has less than 5 diff')\n",
    "        sex_dict[author] = {'female': female,\n",
    "                            'male': male}\n",
    "    elif female > male:\n",
    "        sex_dict[author] = 'female'\n",
    "    elif male > female:\n",
    "        sex_dict[author] = 'male'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32b77fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sexes to data\n",
    "data['sex'] = data.author.apply(lambda x: sex_dict[x])\n",
    "author_wiki_df['sex'] = author_wiki_df.author.apply(lambda x: sex_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f42289",
   "metadata": {},
   "source": [
    "# get google results for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98f9ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scraper(tmp_url_in):\n",
    "    tmp_text = ''\n",
    "    try:\n",
    "        content = requests.get(tmp_url_in, timeout=10)\n",
    "        soup = BeautifulSoup(content.text, 'html.parser')\n",
    "\n",
    "        tmp_text = soup.findAll('p') \n",
    "\n",
    "        tmp_text = [word.text for word in tmp_text]\n",
    "        tmp_text = ' '.join(tmp_text)\n",
    "        tmp_text = re.sub('\\W+', ' ', re.sub('xa0', ' ', tmp_text))\n",
    "    except requests.exceptions.Timeout:\n",
    "        pass\n",
    "    return tmp_text\n",
    "\n",
    "def fetch_urls(query_tmp, cnt):\n",
    "    ua = UserAgent()\n",
    "\n",
    "    query = '+'.join(query_tmp.split())\n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(cnt)\n",
    "    response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href = True)\n",
    "            title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "            description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "            if link != '' and title != '' and description != '': \n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "        except:\n",
    "            continue  \n",
    "\n",
    "    to_remove = []\n",
    "    clean_links = []\n",
    "    for i, l in enumerate(links):\n",
    "        clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
    "        if clean is None:\n",
    "            to_remove.append(i)\n",
    "            continue\n",
    "        clean_links.append(clean.group(1))\n",
    "\n",
    "    return clean_links\n",
    "\n",
    "def write_crawl_results(my_query, the_cnt_in):\n",
    "    tmp_pd = pd.DataFrame()       \n",
    "    for q_blah in my_query:\n",
    "        the_urls_list = fetch_urls(q_blah, the_cnt_in)\n",
    "\n",
    "        for word in the_urls_list:\n",
    "            tmp_txt = my_scraper(word)\n",
    "            if len(tmp_txt) != 0:\n",
    "                try:\n",
    "                    tmp_pd = tmp_pd.append(\n",
    "                        {\n",
    "                            'body_basic': tmp_txt,\n",
    "                            'author': q_blah.replace('philosopher ', '').replace(' review',''),\n",
    "                            'link': word\n",
    "                        },\n",
    "                        ignore_index=True)\n",
    "                except:\n",
    "                    pass\n",
    "    return tmp_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0e41ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_tmp = \"philosopher marcus aurelius review\"\n",
    "query = '+'.join(query_tmp.split())\n",
    "google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + '15'\n",
    "ua = UserAgent()\n",
    "response = requests.get(google_url, {\"User-Agent\": ua.random})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f57acff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "google_data = pd.DataFrame([])\n",
    "for author in author_list:\n",
    "    google_data = google_data.append(write_crawl_results([f\"philosopher {author} review\"], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7820ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_data = google_data[~google_data.author.isin(missing_authors_final)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84ad9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_data[\"sentence_lowered\"] = google_data.body_basic.apply(\n",
    "    lambda x: x.lower()\n",
    ")\n",
    "google_data[\"clean_text\"] = google_data.sentence_lowered.apply(clean_text)\n",
    "google_data[\"rem_sw\"] = google_data.clean_text.apply(rem_sw)\n",
    "google_data[\"rem_sw_stem\"] = google_data.rem_sw.apply(stem_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bd0cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_data['sex'] = google_data.author.apply(lambda x: sex_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34237326",
   "metadata": {},
   "source": [
    "# Run Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28008cab",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f48b1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_senti(arbitrary_text):\n",
    "    '''\n",
    "    Tokenizes arbitrary text and compares each token with the positive and \n",
    "    negative lexicons of each dictionary and outputs the sentiment score, S\n",
    "    '''\n",
    "    arbitrary_text_clean = re.sub(r'[^a-zA-Z ]+', '', arbitrary_text)\n",
    "    arbitrary_text_list = arbitrary_text_clean.split()\n",
    "    \n",
    "    pw = [-1 for word in arbitrary_text_list if word in (pos_neg_dict['negative-words'])]\n",
    "    nw = [1 for word in arbitrary_text_list if word in (pos_neg_dict['positive-words'])]\n",
    "    pc = len(pw)\n",
    "    nc = len(nw)\n",
    "    total = pc + nc\n",
    "    try:\n",
    "        S = (sum(pw) + sum(nw)) / total\n",
    "    except ZeroDivisionError:\n",
    "        S = None\n",
    "    return S\n",
    "def gen_textblob_senti(var_in):\n",
    "    blob = TextBlob(var_in)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d76c3",
   "metadata": {},
   "source": [
    "Apply to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06fff9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sentence_lowered\"] = data.sentence_str.apply(\n",
    "    lambda x: x.lower()\n",
    ")\n",
    "data[\"clean_text\"] = data.sentence_lowered.apply(clean_text)\n",
    "data[\"rem_sw\"] = data.clean_text.apply(rem_sw)\n",
    "data[\"rem_sw_stem\"] = data.rem_sw.apply(stem_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "162641a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and save sentiment dictionaries into pos_neg_dict\n",
    "file_names = ['positive-words', 'negative-words']\n",
    "pos_neg_dict = {}\n",
    "for file in file_names:\n",
    "    path = \"../data/{}.txt\".format(\n",
    "        file\n",
    "    )\n",
    "    with open(path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        contents = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            contents.append(line)\n",
    "    f.close()\n",
    "    pos_neg_dict[file] = contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7f151ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['simple_senti'] = data.rem_sw_stem.apply(gen_senti)\n",
    "data['textblob_senti'] = data.rem_sw_stem.apply(gen_textblob_senti)\n",
    "\n",
    "vaderSent = SentimentIntensityAnalyzer()\n",
    "data['vader'] = data.rem_sw_stem.apply(\n",
    "    lambda x: vaderSent.polarity_scores(x)['compound']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62760bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_output(data, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ca4e8",
   "metadata": {},
   "source": [
    "Apply to author wiki data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a9fbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_wiki_df['simple_senti'] = author_wiki_df.rem_sw_stem.apply(gen_senti)\n",
    "author_wiki_df['textblob_senti'] = author_wiki_df.rem_sw_stem.apply(gen_textblob_senti)\n",
    "\n",
    "vaderSent = SentimentIntensityAnalyzer()\n",
    "author_wiki_df['vader'] = author_wiki_df.rem_sw_stem.apply(\n",
    "    lambda x: vaderSent.polarity_scores(x)['compound']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51540463",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_output(author_wiki_df, 'author_wiki_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a78898",
   "metadata": {},
   "source": [
    "Apply to google data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "712f1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_data['simple_senti'] = google_data.rem_sw_stem.apply(gen_senti)\n",
    "google_data['textblob_senti'] = google_data.rem_sw_stem.apply(gen_textblob_senti)\n",
    "\n",
    "vaderSent = SentimentIntensityAnalyzer()\n",
    "google_data['vader'] = google_data.rem_sw_stem.apply(\n",
    "    lambda x: vaderSent.polarity_scores(x)['compound']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9adc1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_output(google_data, 'google_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e569de",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ef819da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lda_fun(df_in, n_topics_in, num_words_in):\n",
    "#     data_tmp = df_in.str.split()\n",
    "#     id2word = corpora.Dictionary(data_tmp)\n",
    "    \n",
    "#     corpus = [id2word.doc2bow(text) for text in data_tmp]\n",
    "\n",
    "#     ldamodel = gensim.models.ldamodel.LdaModel(\n",
    "#         corpus, num_topics=n_topics_in, id2word=id2word, passes=15)\n",
    "# #     ldamodel.save('model5.gensim')\n",
    "#     topics = ldamodel.print_topics(num_words=num_words_in)\n",
    "# #     print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  \n",
    "#     coherence_model_lda = CoherenceModel(\n",
    "#         model=ldamodel, texts=data_tmp, dictionary=id2word, coherence='c_v')\n",
    "#     coherence_lda = coherence_model_lda.get_coherence()\n",
    "# #     print('\\nCoherence Score: ', coherence_lda)\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "#     return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56e3f124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lda_author_topics_dict = {}\n",
    "\n",
    "# for author in author_list:\n",
    "#     print(f'running {author}')\n",
    "#     author_data = data[data['author'] == author]\n",
    "#     author_data_drop_na = author_data[~author_data.rem_sw_stem.isna()]\n",
    "#     the_topics = lda_fun(author_data_drop_na.rem_sw_stem, 2, 5)\n",
    "#     lda_author_topics_dict[author] = the_topics\n",
    "\n",
    "# # write lda_author_topics_dict to output\n",
    "# lda_author_topics_df_raw = pd.DataFrame(lda_author_topics_dict)\n",
    "# lda_author_topics_df = lda_author_topics_df_raw.unstack().reset_index().drop('level_1', axis=1).rename(\n",
    "#     columns = {'level_0':'author',\n",
    "#                '0':'topic'}\n",
    "# )\n",
    "\n",
    "# lda_author_topics_df['sex'] = lda_author_topics_df.author.apply(lambda x: sex_dict[x])\n",
    "\n",
    "# write_to_output(lda_author_topics_df, 'lda_author_topics_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "169e92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_school_topics_dict = {}\n",
    "# for school in school_list:\n",
    "#     print(f'running {school}')\n",
    "#     the_topics = lda_fun(data[data['school'] == school].rem_sw_stem, 2, 5)\n",
    "#     lda_school_topics_dict[school] = the_topics\n",
    "\n",
    "# lda_school_topics_df_raw = pd.DataFrame(lda_school_topics_dict)\n",
    "# lda_school_topics_df = lda_school_topics_df_raw.unstack().reset_index().drop('level_1', axis=1).rename(\n",
    "#     columns = {'level_0':'author',\n",
    "#                '0':'topic'}\n",
    "# )\n",
    "\n",
    "\n",
    "# lda_school_topics_df.to_csv('../output/lda_school_topics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f584ec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lda_author_wiki_topics_dict = {}\n",
    "\n",
    "# for author in author_list:\n",
    "#     print(f'running {author}')\n",
    "#     author_wiki_data = author_wiki_df[author_wiki_df['author'] == author]\n",
    "#     author_wiki_data_drop_na = author_wiki_data[~author_wiki_data.rem_sw_stem.isna()]\n",
    "#     the_topics = lda_fun(author_wiki_data_drop_na.rem_sw_stem, 2, 5)\n",
    "#     lda_author_wiki_topics_dict[author] = the_topics\n",
    "\n",
    "# lda_author_wiki_topics_df_raw = pd.DataFrame(lda_author_wiki_topics_dict)\n",
    "# lda_author_wiki_topics_df = lda_author_wiki_topics_df_raw.unstack().reset_index().drop('level_1', axis=1).rename(\n",
    "#     columns = {'level_0':'author',\n",
    "#                '0':'topic'}\n",
    "# )\n",
    "\n",
    "# lda_author_wiki_topics_df['sex'] = lda_author_wiki_topics_df.author.apply(lambda x: sex_dict[x])\n",
    "# write_to_output(lda_author_wiki_topics_df, 'lda_author_wiki_topics_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ef81d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lda_author_google_topics_dict = {}\n",
    "\n",
    "# for author in author_list:\n",
    "#     print(f'running {author}')\n",
    "#     google_author_data = google_data[google_data['author'] == author]\n",
    "#     google_author_data_drop_na = google_author_data[~google_author_data.rem_sw_stem.isna()]\n",
    "#     the_topics = lda_fun(google_author_data_drop_na.rem_sw_stem, 2, 5)\n",
    "#     lda_author_google_topics_dict[author] = the_topics\n",
    "\n",
    "# # write lda_author_google_topics_df to output\n",
    "# lda_author_google_topics_df_raw = pd.DataFrame(lda_author_google_topics_dict)\n",
    "# lda_author_google_topics_df = lda_author_google_topics_df_raw.unstack().reset_index().drop('level_1', axis=1).rename(\n",
    "#     columns = {'level_0':'author',\n",
    "#                '0':'topic'}\n",
    "# )\n",
    "# lda_author_google_topics_df['sex'] = lda_author_google_topics_df.author.apply(lambda x: sex_dict[x])\n",
    "# write_to_output(lda_author_google_topics_df, 'lda_author_google_topics_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcd988",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2421ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.options.display.float_format = '{:,}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0953da2",
   "metadata": {},
   "source": [
    "### Descriptive figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "300cc13f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_dates = data[['author','original_publication_date']].groupby(['author']).max('original_publication_date').join(\n",
    "    data[['author','original_publication_date']].groupby(['author']).min('original_publication_date'),\n",
    "    lsuffix = '_max',\n",
    "    rsuffix = '_min'\n",
    ")\n",
    "\n",
    "author_dates.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8408e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dates['sex'] = author_dates.author.apply(lambda x: sex_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c71887d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_descriptive_counts(**kwargs):\n",
    "    count_records = pd.DataFrame(data.groupby(['sex']).size(), columns = ['records'])\n",
    "    count_unique_authors = pd.DataFrame(data.groupby('sex')['author'].nunique())\n",
    "    count_unique_titles = pd.DataFrame(data.groupby('sex')['title'].nunique())\n",
    "    count_unique_schools = pd.DataFrame(data.groupby('sex')['school'].nunique())\n",
    "    \n",
    "    descriptive_cts_df = count_unique_authors.join(count_unique_titles).join(count_records).join(count_unique_schools)\n",
    "    descriptive_cts_df.columns = ['authors','titles','records', 'schools']\n",
    "    \n",
    "    display(descriptive_cts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a931dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_descriptive_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c5570bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# men do all but one school (feminism) and women only do one school (feminism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a4386aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names = list(author_dates[author_dates.sex=='male']['author'])\n",
    "male_dates = list(author_dates[(author_dates.sex=='male')]['original_publication_date_min'])\n",
    "\n",
    "female_names = list(author_dates[author_dates.sex=='female']['author'])\n",
    "female_dates = list(author_dates[(author_dates.sex=='female')]['original_publication_date_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a53fa4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timelines(**kwargs):\n",
    "    ### MALES ###\n",
    "    levels = np.tile([-5, 5, -3, 3, -1, -1],\n",
    "                     int(np.ceil(len(male_dates)/6)))[:len(male_dates)]\n",
    "\n",
    "    # Create figure and plot a stem plot with the date\n",
    "    f,(ax,ax2) = plt.subplots(1,2,\n",
    "                              sharey=True,\n",
    "                              facecolor='w',\n",
    "                              figsize=(20, 8),\n",
    "                              constrained_layout=True)\n",
    "\n",
    "    f.suptitle(\"Male Author First Original Publication Date\", fontsize=25)\n",
    "\n",
    "    ax.vlines(male_dates, 0, levels, color=\"tab:blue\")  # The vertical stems.\n",
    "    ax.plot(male_dates,\n",
    "            np.zeros_like(male_dates),\n",
    "            \"-o\",\n",
    "            color=\"k\",\n",
    "            markerfacecolor=\"w\")  # Baseline and markers on it.\n",
    "    ax.set_xlim(-400,500)\n",
    "\n",
    "    ax2.vlines(male_dates, 0, levels, color=\"tab:blue\")  # The vertical stems.\n",
    "    ax2.plot(male_dates,\n",
    "            np.zeros_like(male_dates),\n",
    "            \"-o\",\n",
    "            color=\"k\",\n",
    "            markerfacecolor=\"w\")  # Baseline and markers on it.\n",
    "    ax2.set_xlim(1500,2020)\n",
    "\n",
    "\n",
    "    # annotate lines\n",
    "    for d, l, r in zip(male_dates, levels, male_names):\n",
    "        ax.annotate(r, xy=(d, l),\n",
    "                    xytext=(-3, np.sign(l)*3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    rotation=60,\n",
    "                    horizontalalignment=\"right\",\n",
    "                    verticalalignment=\"bottom\" if l > 0 else \"top\")\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "\n",
    "    for d, l, r in zip(male_dates, levels, male_names):\n",
    "        ax2.annotate(r, xy=(d, l),\n",
    "                    xytext=(-3, np.sign(l)*3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    rotation=90,\n",
    "                    horizontalalignment=\"right\",\n",
    "                    verticalalignment=\"bottom\" if l > 0 else \"top\")\n",
    "\n",
    "    plt.setp(ax2.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "\n",
    "    # CITATION: https://stackoverflow.com/questions/32185411/break-in-x-axis-of-matplotlib\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.spines[[\"left\", \"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "    ax2.yaxis.set_visible(False)\n",
    "    ax2.spines[[\"left\", \"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "    ax.margins(y=0.1)\n",
    "    ax2.margins(y=0.1)\n",
    "\n",
    "    d = .015 \n",
    "\n",
    "    kwargs = dict(transform=ax.transAxes, color='k', clip_on=False)\n",
    "    ax.plot((1-d,1+d), (-d,+d), **kwargs)\n",
    "    # ax.plot((1-d,1+d),(1-d,1+d), **kwargs)\n",
    "\n",
    "    kwargs.update(transform=ax2.transAxes)  # switch to the bottom axes\n",
    "    # ax2.plot((-d,+d), (1-d,1+d), **kwargs)\n",
    "    ax2.plot((-d,+d), (-d,+d), **kwargs)\n",
    "\n",
    "    plt.savefig('../figs/male_timeline_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "    ### FEMALES ###\n",
    "    levels = np.tile([-5, 5, -3, 3, -1, -1],\n",
    "                     int(np.ceil(len(female_dates)/6)))[:len(female_dates)]\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(8, 6),\n",
    "        constrained_layout=True\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\"Female Author First Original Publication Date\", fontsize=16)\n",
    "\n",
    "    ax.vlines(female_dates, 0, levels, color=\"tab:pink\")  # The vertical stems.\n",
    "    ax.plot(female_dates,\n",
    "            np.zeros_like(female_dates),\n",
    "            \"-o\",\n",
    "            color=\"k\",\n",
    "            markerfacecolor=\"w\")  # Baseline and markers on it.\n",
    "    ax.set_xlim(1700, 2000)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.spines[[\"left\", \"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "    # annotate lines\n",
    "    for d, l, r in zip(female_dates, levels, female_names):\n",
    "        ax.annotate(r, xy=(d, l),\n",
    "                    xytext=(-3, np.sign(l)*3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    rotation=0,\n",
    "                    horizontalalignment=\"right\",\n",
    "                    verticalalignment=\"bottom\" if l > 0 else \"top\")\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "    plt.savefig('../figs/female_timeline_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f28c1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_timelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c525c",
   "metadata": {},
   "source": [
    "### sentiment by gender (for works and wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f78ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_senti_scores_data(**kwargs):\n",
    "    ### OG DATA ###\n",
    "    labels = ['simple senti', 'vader senti', 'textblob senti']\n",
    "    gpd_data = data.groupby(['sex']).mean()[['simple_senti','vader','textblob_senti']].reset_index()\n",
    "    \n",
    "    men_means = list(round(gpd_data[gpd_data.sex=='male'][['simple_senti','vader','textblob_senti']].iloc[0],4))\n",
    "    women_means = list(round(gpd_data[gpd_data.sex=='female'][['simple_senti','vader','textblob_senti']].iloc[0],4))\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "    rects1 = ax.bar(x - width/2, men_means, width, label='Men', color = 'blue')\n",
    "    rects2 = ax.bar(x + width/2, women_means, width, label='Women', color = 'pink')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('philosophy data sentiment scores', size = 16)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.bar_label(rects1, padding=3)\n",
    "    ax.bar_label(rects2, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('../figs/philosophy_senti_scores.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_senti_scores_wiki(**kwargs):\n",
    "    ### WIKI DATA ###\n",
    "    labels = ['simple senti', 'vader senti', 'textblob senti']\n",
    "    gpd_data = author_wiki_df.groupby(['sex']).mean()[['simple_senti','vader','textblob_senti']].reset_index()\n",
    "    \n",
    "    men_means = list(round(gpd_data[gpd_data.sex=='male'][['simple_senti','vader','textblob_senti']].iloc[0],4))\n",
    "    women_means = list(round(gpd_data[gpd_data.sex=='female'][['simple_senti','vader','textblob_senti']].iloc[0],4))\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    rects1 = ax.bar(x - width/2, men_means, width, label='Men', color = 'blue')\n",
    "    rects2 = ax.bar(x + width/2, women_means, width, label='Women', color = 'pink')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('wikipedia data sentiment scores', size = 16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.bar_label(rects1, padding=3)\n",
    "    ax.bar_label(rects2, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('../figs/wikipedia_senti_scores.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_senti_scores_google(**kwargs):\n",
    "    ### GOOGLE DATA ###\n",
    "    labels = ['simple senti', 'vader senti', 'textblob senti']\n",
    "    gpd_data = google_data.groupby(['sex']).mean()[['simple_senti','vader','textblob_senti']].reset_index()\n",
    "    \n",
    "    men_means = list(round(gpd_data[gpd_data.sex=='male'][['simple_senti','vader','textblob_senti']].iloc[0],4))\n",
    "    women_means = list(round(gpd_data[gpd_data.sex=='female'][['simple_senti','vader','textblob_senti']].iloc[0],4))\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    rects1 = ax.bar(x - width/2, men_means, width, label='Men', color = 'blue')\n",
    "    rects2 = ax.bar(x + width/2, women_means, width, label='Women', color = 'pink')\n",
    "\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('google data sentiment scores', size = 16)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.bar_label(rects1, padding=3)\n",
    "    ax.bar_label(rects2, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('../figs/google_senti_scores.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c56bf5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_data(**kwargs):    \n",
    "    fig, ax = plt.subplots(1,2,figsize =(10, 7))\n",
    "\n",
    "    bx_data_men = [(data[data.sex == 'male'].dropna(subset=['simple_senti']).simple_senti),\n",
    "               (data[data.sex == 'male'].dropna(subset=['textblob_senti']).textblob_senti),\n",
    "               (data[data.sex == 'male'].dropna(subset=['vader']).vader)]\n",
    "\n",
    "    bx_data_women = [(data[data.sex == 'female'].dropna(subset=['simple_senti']).simple_senti),\n",
    "               (data[data.sex == 'female'].dropna(subset=['textblob_senti']).textblob_senti),\n",
    "               (data[data.sex == 'female'].dropna(subset=['vader']).vader)]\n",
    "\n",
    "    # Creating plot\n",
    "    labels = ['simple senti','textblob senti','vader']\n",
    "\n",
    "    ax[0].set_xticklabels(labels)\n",
    "    ax[0].set_title('men', size = 16)\n",
    "\n",
    "    ax[1].set_xticklabels(labels)\n",
    "    ax[1].set_title('women', size = 16)\n",
    "\n",
    "    plt.suptitle('Sentiment Scores for Philosophy Text Data', size = 16)\n",
    "\n",
    "    ax[0].boxplot(bx_data_men,\n",
    "                capprops=dict(color='blue'),\n",
    "                whiskerprops=dict(color='blue'),\n",
    "                medianprops=dict(color='blue'))\n",
    "\n",
    "    ax[1].boxplot(bx_data_women,\n",
    "                capprops=dict(color='pink'),\n",
    "                whiskerprops=dict(color='pink'),\n",
    "                medianprops=dict(color='pink'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3c7b4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_wiki(**kwargs):    \n",
    "    fig, ax = plt.subplots(1,2,figsize =(10, 7))\n",
    "\n",
    "    bx_data_men = [(author_wiki_df[author_wiki_df.sex == 'male'].dropna(subset=['simple_senti']).simple_senti),\n",
    "               (author_wiki_df[author_wiki_df.sex == 'male'].dropna(subset=['textblob_senti']).textblob_senti),\n",
    "               (author_wiki_df[author_wiki_df.sex == 'male'].dropna(subset=['vader']).vader)]\n",
    "\n",
    "    bx_data_women = [(author_wiki_df[author_wiki_df.sex == 'female'].dropna(subset=['simple_senti']).simple_senti),\n",
    "               (author_wiki_df[author_wiki_df.sex == 'female'].dropna(subset=['textblob_senti']).textblob_senti),\n",
    "               (author_wiki_df[author_wiki_df.sex == 'female'].dropna(subset=['vader']).vader)]\n",
    "\n",
    "    # Creating plot\n",
    "    labels = ['simple senti','textblob senti','vader']\n",
    "\n",
    "    ax[0].set_xticklabels(labels)\n",
    "    ax[0].set_title('men', size = 16)\n",
    "\n",
    "    ax[1].set_xticklabels(labels)\n",
    "    ax[1].set_title('women', size = 16)\n",
    "\n",
    "    plt.suptitle('Sentiment Scores for Wikipedia Data', size = 16)\n",
    "\n",
    "    ax[0].boxplot(bx_data_men,\n",
    "                capprops=dict(color='blue'),\n",
    "                whiskerprops=dict(color='blue'),\n",
    "                medianprops=dict(color='blue'))\n",
    "\n",
    "    ax[1].boxplot(bx_data_women,\n",
    "                capprops=dict(color='pink'),\n",
    "                whiskerprops=dict(color='pink'),\n",
    "                medianprops=dict(color='pink'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2542af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_google(**kwargs):    \n",
    "    fig, ax = plt.subplots(1,2,figsize =(10, 7))\n",
    "\n",
    "    bx_data_men = [(google_data[google_data.sex == 'male'].dropna(subset=['simple_senti']).simple_senti),\n",
    "               (google_data[google_data.sex == 'male'].dropna(subset=['textblob_senti']).textblob_senti),\n",
    "               (google_data[google_data.sex == 'male'].dropna(subset=['vader']).vader)]\n",
    "\n",
    "    bx_data_women = [(google_data[google_data.sex == 'female'].dropna(subset=['simple_senti']).simple_senti),\n",
    "               (google_data[google_data.sex == 'female'].dropna(subset=['textblob_senti']).textblob_senti),\n",
    "               (google_data[google_data.sex == 'female'].dropna(subset=['vader']).vader)]\n",
    "\n",
    "    # Creating plot\n",
    "    labels = ['simple senti','textblob senti','vader']\n",
    "\n",
    "    ax[0].set_xticklabels(labels)\n",
    "    ax[0].set_title('men', size = 16)\n",
    "\n",
    "    ax[1].set_xticklabels(labels)\n",
    "    ax[1].set_title('women', size = 16)\n",
    "\n",
    "    plt.suptitle('Sentiment Scores for Google Data', size = 16)\n",
    "\n",
    "    ax[0].boxplot(bx_data_men,\n",
    "                capprops=dict(color='blue'),\n",
    "                whiskerprops=dict(color='blue'),\n",
    "                medianprops=dict(color='blue'))\n",
    "\n",
    "    ax[1].boxplot(bx_data_women,\n",
    "                capprops=dict(color='pink'),\n",
    "                whiskerprops=dict(color='pink'),\n",
    "                medianprops=dict(color='pink'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2926cfd7",
   "metadata": {},
   "source": [
    "### wordclouds of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f848d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_data(**kwargs):\n",
    "    women_text = ' '.join(list(data[(data.sex=='female') & (~data.rem_sw.isna())].rem_sw))\n",
    "    men_text = ' '.join(list(data[(data.sex=='male') & (~data.rem_sw.isna())].rem_sw))\n",
    "\n",
    "    women_coloring = np.array(Image.open(\"../figs/women.png\"))\n",
    "    men_coloring = np.array(Image.open(\"../figs/men2.png\"))\n",
    "\n",
    "\n",
    "    wc_women = WordCloud(background_color=\"white\",\n",
    "                         max_words=200,\n",
    "                         mask=women_coloring,\n",
    "                         max_font_size=50,\n",
    "                         random_state=123)\n",
    "    wc_men = WordCloud(background_color=\"white\",\n",
    "                         max_words=1000,\n",
    "                         mask=men_coloring,\n",
    "                         max_font_size=50,\n",
    "                         random_state=123)\n",
    "    # generate word cloud\n",
    "    wc_women.generate(women_text)\n",
    "    wc_men.generate(men_text)\n",
    "\n",
    "    # create coloring from image\n",
    "    women_image_colors = ImageColorGenerator(women_coloring)\n",
    "    men_image_colors = ImageColorGenerator(men_coloring)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,20))\n",
    "\n",
    "    ax[0].imshow(wc_women.recolor(color_func=women_image_colors), interpolation=\"bilinear\")\n",
    "    ax[0].set_axis_off()\n",
    "\n",
    "    ax[1].imshow(wc_men.recolor(color_func=men_image_colors), interpolation=\"bilinear\")\n",
    "    ax[1].set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0beeed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c26fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_wikipedia(**kwargs):    \n",
    "    women_text = ' '.join(list(author_wiki_df[(author_wiki_df.sex=='female') & (~author_wiki_df.rem_sw.isna())].rem_sw))\n",
    "    men_text = ' '.join(list(author_wiki_df[(author_wiki_df.sex=='male') & (~author_wiki_df.rem_sw.isna())].rem_sw))\n",
    "\n",
    "    women_coloring = np.array(Image.open(\"../figs/women.png\"))\n",
    "    men_coloring = np.array(Image.open(\"../figs/men2.png\"))\n",
    "\n",
    "    wc_women = WordCloud(background_color=\"white\",\n",
    "                         max_words=250,\n",
    "                         mask=women_coloring,\n",
    "                         max_font_size=50,\n",
    "                         random_state=123)\n",
    "    wc_men = WordCloud(background_color=\"white\",\n",
    "                         max_words=1000,\n",
    "                         mask=men_coloring,\n",
    "                         max_font_size=50,\n",
    "                         random_state=123)\n",
    "    # generate word cloud\n",
    "    wc_women.generate(women_text)\n",
    "    wc_men.generate(men_text)\n",
    "\n",
    "    # create coloring from image\n",
    "    women_image_colors = ImageColorGenerator(women_coloring)\n",
    "    men_image_colors = ImageColorGenerator(men_coloring)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,20))\n",
    "\n",
    "    ax[0].imshow(wc_women.recolor(color_func=women_image_colors), interpolation=\"bilinear\")\n",
    "    ax[0].set_axis_off()\n",
    "\n",
    "    ax[1].imshow(wc_men.recolor(color_func=men_image_colors), interpolation=\"bilinear\")\n",
    "    ax[1].set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa099767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud_wikipedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87d5d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_google(**kwargs):    \n",
    "    women_text = ' '.join(list(author_wiki_df[(author_wiki_df.sex=='female') & (~author_wiki_df.rem_sw.isna())].rem_sw))\n",
    "    men_text = ' '.join(list(author_wiki_df[(author_wiki_df.sex=='male') & (~author_wiki_df.rem_sw.isna())].rem_sw))\n",
    "\n",
    "    women_coloring = np.array(Image.open(\"../figs/women.png\"))\n",
    "    men_coloring = np.array(Image.open(\"../figs/men2.png\"))\n",
    "\n",
    "    wc_women = WordCloud(background_color=\"white\",\n",
    "                         max_words=250,\n",
    "                         mask=women_coloring,\n",
    "                         max_font_size=50,\n",
    "                         random_state=123)\n",
    "    wc_men = WordCloud(background_color=\"white\",\n",
    "                         max_words=1000,\n",
    "                         mask=men_coloring,\n",
    "                         max_font_size=50,\n",
    "                         random_state=123)\n",
    "    # generate word cloud\n",
    "    wc_women.generate(women_text)\n",
    "    wc_men.generate(men_text)\n",
    "\n",
    "    # create coloring from image\n",
    "    women_image_colors = ImageColorGenerator(women_coloring)\n",
    "    men_image_colors = ImageColorGenerator(men_coloring)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,20))\n",
    "\n",
    "    ax[0].imshow(wc_women.recolor(color_func=women_image_colors), interpolation=\"bilinear\")\n",
    "    ax[0].set_axis_off()\n",
    "\n",
    "    ax[1].imshow(wc_men.recolor(color_func=men_image_colors), interpolation=\"bilinear\")\n",
    "    ax[1].set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc99b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud_google()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b76ecf",
   "metadata": {},
   "source": [
    "# overlap between genders (venn diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "203b151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def venn_wordcloud_data(**kwargs):\n",
    "    women_rm_sw_text = ' '.join(list(data[(data.sex=='female') & (~data.rem_sw.isna())].rem_sw))\n",
    "    men_rm_sw_text = ' '.join(list(data[(data.sex=='male') & (~data.rem_sw.isna())].rem_sw))\n",
    "\n",
    "    women_splt = set(women_rm_sw_text.split())\n",
    "    men_splt = set(men_rm_sw_text.split())\n",
    "    just_women = set([x for x in women_splt if x not in men_splt])\n",
    "    just_men = set([x for x in men_splt if x not in women_splt])\n",
    "    men_and_women = just_women.intersection(just_men)\n",
    "\n",
    "    women_fnl = just_women.union(men_and_women)\n",
    "    men_fnl = just_men.union(men_and_women)\n",
    "\n",
    "    def color_func(word, *args, **kwargs):\n",
    "        if word in just_men:\n",
    "            return \"#0000ff\" # blue1\n",
    "        elif word in just_women:\n",
    "            return \"#FA33FF\" # pink\n",
    "        else:\n",
    "            return \"#737373\" # gray6 (aka off-black)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(25, 25))\n",
    "    fig.suptitle(\"Philosophy Texts\", fontsize=25)\n",
    "\n",
    "    sets = []\n",
    "    for string in [women_rm_sw_text, men_rm_sw_text]:\n",
    "\n",
    "        # get a word list\n",
    "        words = string.split(' ')\n",
    "\n",
    "        # remove non alphanumeric characters\n",
    "        words = [''.join(ch for ch in word if ch.isalnum()) for word in words]\n",
    "\n",
    "        # convert to all lower case\n",
    "        words = [word.lower() for word in words]\n",
    "\n",
    "        sets.append(set(words))\n",
    "\n",
    "    # create visualisation\n",
    "    out = venn2_wordcloud(sets, wordcloud_kwargs=dict(color_func=color_func), ax=ax)\n",
    "    out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "241f8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# venn_wordcloud_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a06dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def venn_wordcloud_wiki(**kwargs):\n",
    "    women_rm_sw_text = ' '.join(list(author_wiki_df[(author_wiki_df.sex=='female') & (~author_wiki_df.rem_sw.isna())].rem_sw))\n",
    "    men_rm_sw_text = ' '.join(list(author_wiki_df[(author_wiki_df.sex=='male') & (~author_wiki_df.rem_sw.isna())].rem_sw))\n",
    "\n",
    "    women_splt = set(women_rm_sw_text.split())\n",
    "    men_splt = set(men_rm_sw_text.split())\n",
    "    just_women = set([x for x in women_splt if x not in men_splt])\n",
    "    just_men = set([x for x in men_splt if x not in women_splt])\n",
    "    men_and_women = just_women.intersection(just_men)\n",
    "\n",
    "    women_fnl = just_women.union(men_and_women)\n",
    "    men_fnl = just_men.union(men_and_women)\n",
    "\n",
    "    def color_func(word, *args, **kwargs):\n",
    "        if word in just_men:\n",
    "            return \"#0000ff\" # blue1\n",
    "        elif word in just_women:\n",
    "            return \"#FA33FF\" # pink\n",
    "        else:\n",
    "            return \"#737373\" # gray6 (aka off-black)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(25, 25))\n",
    "    fig.suptitle(\"Wikipedia Bios\", fontsize=25)\n",
    "\n",
    "\n",
    "    sets = []\n",
    "    for string in [women_rm_sw_text, men_rm_sw_text]:\n",
    "\n",
    "        # get a word list\n",
    "        words = string.split(' ')\n",
    "\n",
    "        # remove non alphanumeric characters\n",
    "        words = [''.join(ch for ch in word if ch.isalnum()) for word in words]\n",
    "\n",
    "        # convert to all lower case\n",
    "        words = [word.lower() for word in words]\n",
    "\n",
    "        sets.append(set(words))\n",
    "\n",
    "    # create visualisation\n",
    "    out = venn2_wordcloud(sets, wordcloud_kwargs=dict(color_func=color_func), ax=ax)\n",
    "    out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f81d5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# venn_wordcloud_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f938bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def venn_wordcloud_google(**kwargs):\n",
    "    women_rm_sw_text = ' '.join(list(google_data[(google_data.sex=='female') & (~google_data.rem_sw.isna())].rem_sw))\n",
    "    men_rm_sw_text = ' '.join(list(google_data[(google_data.sex=='male') & (~google_data.rem_sw.isna())].rem_sw))\n",
    "\n",
    "    women_splt = set(women_rm_sw_text.split())\n",
    "    men_splt = set(men_rm_sw_text.split())\n",
    "    just_women = set([x for x in women_splt if x not in men_splt])\n",
    "    just_men = set([x for x in men_splt if x not in women_splt])\n",
    "    men_and_women = just_women.intersection(just_men)\n",
    "\n",
    "    women_fnl = just_women.union(men_and_women)\n",
    "    men_fnl = just_men.union(men_and_women)\n",
    "\n",
    "    def color_func(word, *args, **kwargs):\n",
    "        if word in just_men:\n",
    "            return \"#0000ff\" # blue1\n",
    "        elif word in just_women:\n",
    "            return \"#FA33FF\" # pink\n",
    "        else:\n",
    "            return \"#737373\" # gray\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(25, 25))\n",
    "    fig.suptitle(\"Google Search Results\", fontsize=25)\n",
    "\n",
    "    # venn2_wordcloud(sets, wordcloud_kwargs=dict(max_font_size=50, min_font_size=30), ax=ax2)\n",
    "    # ax2.set_title('cloud')\n",
    "\n",
    "    sets = []\n",
    "    for string in [women_rm_sw_text, men_rm_sw_text]:\n",
    "\n",
    "        # get a word list\n",
    "        words = string.split(' ')\n",
    "\n",
    "        # remove non alphanumeric characters\n",
    "        words = [''.join(ch for ch in word if ch.isalnum()) for word in words]\n",
    "\n",
    "        # convert to all lower case\n",
    "        words = [word.lower() for word in words]\n",
    "\n",
    "        sets.append(set(words))\n",
    "\n",
    "    # create visualisation\n",
    "    out = venn2_wordcloud(sets, wordcloud_kwargs=dict(color_func=color_func), ax=ax)\n",
    "    out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72450097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# venn_wordcloud_google()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
